{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZ4nO/7nEa9CiHFZqUPwAQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saranpan/guilding-nb/blob/main/thai_tokenization_evolution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we are going to start from the simpliest to the most complicated but efficient"
      ],
      "metadata": {
        "id": "5zvPY5xL2wTD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Status:\n",
        "\n",
        "✅ Done\n",
        "❌ Not yet"
      ],
      "metadata": {
        "id": "N4_0f7JGFeIm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅ Greedy First vocab match"
      ],
      "metadata": {
        "id": "LETK3UAWwL08"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RX3o8mDwvaEQ"
      },
      "outputs": [],
      "source": [
        "# Predefined Vocab\n",
        "\n",
        "vocab = ['กิน','ข้าว','อร่อย', 'ผม']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_txt = 'ผมกินข้าว'"
      ],
      "metadata": {
        "id": "dE4D2Ti7vlwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ถ้าไม่เจอ token ที่อยู่ใน vocab จะให้เป็น `<unk>` แทน"
      ],
      "metadata": {
        "id": "mUuPq7PE8aCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = ['กิน','ข้าว','อร่อย', 'ผม']\n",
        "test_txt = 'ผมกินข้าว'\n",
        "\n",
        "token_lst = []\n",
        "skip_ = 1\n",
        "i = 0\n",
        "\n",
        "while i < len(test_txt):  # i : start from word position i_th\n",
        "\n",
        "    for j in range(len(test_txt)-i): # j : try from 0 until the position which match with vocab\n",
        "        \n",
        "        txt = test_txt[i:i+j+1]\n",
        "        print(txt)\n",
        "        if txt in vocab:\n",
        "            token_lst.append(txt)\n",
        "            skip_ = len(txt)\n",
        "            print(f'{txt} : found in vocab')\n",
        "            i += skip_\n",
        "            break\n",
        "        else:\n",
        "            print(f'{txt} : NOT found in vocab')\n",
        "            \n",
        "        if j == len(test_txt)-i:  # Match until the last character and still does not form any of tokens\n",
        "            token_lst.append('<UNK>')\n",
        "            skip_ = len(txt)\n",
        "            print('=== Cannot detected ===')\n",
        "            i += skip_\n",
        "            break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rozAyInxvrmv",
        "outputId": "2d626f5b-4b14-4b73-c1b1-08f91c1bb9c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ผ\n",
            "ผ : NOT found in vocab\n",
            "ผม\n",
            "ผม : found in vocab\n",
            "ก\n",
            "ก : NOT found in vocab\n",
            "กิ\n",
            "กิ : NOT found in vocab\n",
            "กิน\n",
            "กิน : found in vocab\n",
            "ข\n",
            "ข : NOT found in vocab\n",
            "ข้\n",
            "ข้ : NOT found in vocab\n",
            "ข้า\n",
            "ข้า : NOT found in vocab\n",
            "ข้าว\n",
            "ข้าว : found in vocab\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_lst"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qT4UtggdwlDb",
        "outputId": "1a3123e1-cf54-4b67-b119-494e461c4dc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ผม', 'กิน', 'ข้าว']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ลองมา form มาให้เป็น defined function เพื่อ reusable\n",
        "\n",
        "def greedy_match_tokenizer(test_txt,vocab):\n",
        "\n",
        "    token_lst = []\n",
        "    skip_ = 1\n",
        "    i = 0\n",
        "\n",
        "    while i < len(test_txt):  # i : start from word position i_th\n",
        "        \n",
        "        for j in range(len(test_txt)-i): # j : try from 0 until the position which match with vocab\n",
        "            txt = test_txt[i:i+j+1]\n",
        "\n",
        "            if txt in vocab:\n",
        "                token_lst.append(txt)\n",
        "                skip_ = len(txt)\n",
        "                print(f'{txt} : found in vocab \\n')\n",
        "                i += skip_\n",
        "                break\n",
        "            else:\n",
        "                print(f'{txt} : NOT found in vocab')\n",
        "\n",
        "            if j == len(test_txt)-i-1:  # Match until the last character and still does not form any of tokens\n",
        "                token_lst.append('<UNK>')\n",
        "                skip_ = len(txt)\n",
        "                print('=== Cannot detected ===')\n",
        "                i += skip_\n",
        "                break\n",
        "            \n",
        "\n",
        "    return token_lst"
      ],
      "metadata": {
        "id": "mGsswOfqDsPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = ['กิน','ข้าว','อร่อย', 'ผม']\n",
        "test_txt = 'ผมกินข้าว'\n",
        "\n",
        "greedy_match_tokenizer(test_txt,vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVHXXGJ3U8Qc",
        "outputId": "af7e7a24-1689-407f-ff92-38be06c3c8b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ผ : NOT found in vocab\n",
            "ผม : found in vocab \n",
            "\n",
            "ก : NOT found in vocab\n",
            "กิ : NOT found in vocab\n",
            "กิน : found in vocab \n",
            "\n",
            "ข : NOT found in vocab\n",
            "ข้ : NOT found in vocab\n",
            "ข้า : NOT found in vocab\n",
            "ข้าว : found in vocab \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ผม', 'กิน', 'ข้าว']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = ['ฉัน','สยาม','ผม','พารากอน', 'นั่ง','รถ','รถไฟฟ้า','ไฟฟ้า', 'ไป','เมื่อวาน']\n",
        "test_txt = 'ผมนั่งรถไฟฟ้าไปสยามพารากอนเมื่อวาน'\n",
        "\n",
        "greedy_match_tokenizer(test_txt,vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRFVNIoUdGhk",
        "outputId": "cb7792ff-f41e-4781-ba51-5fb55d94aae9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ผ : NOT found in vocab\n",
            "ผม : found in vocab \n",
            "\n",
            "น : NOT found in vocab\n",
            "นั : NOT found in vocab\n",
            "นั่ : NOT found in vocab\n",
            "นั่ง : found in vocab \n",
            "\n",
            "ร : NOT found in vocab\n",
            "รถ : found in vocab \n",
            "\n",
            "ไ : NOT found in vocab\n",
            "ไฟ : NOT found in vocab\n",
            "ไฟฟ : NOT found in vocab\n",
            "ไฟฟ้ : NOT found in vocab\n",
            "ไฟฟ้า : found in vocab \n",
            "\n",
            "ไ : NOT found in vocab\n",
            "ไป : found in vocab \n",
            "\n",
            "ส : NOT found in vocab\n",
            "สย : NOT found in vocab\n",
            "สยา : NOT found in vocab\n",
            "สยาม : found in vocab \n",
            "\n",
            "พ : NOT found in vocab\n",
            "พา : NOT found in vocab\n",
            "พาร : NOT found in vocab\n",
            "พารา : NOT found in vocab\n",
            "พาราก : NOT found in vocab\n",
            "พารากอ : NOT found in vocab\n",
            "พารากอน : found in vocab \n",
            "\n",
            "เ : NOT found in vocab\n",
            "เม : NOT found in vocab\n",
            "เมื : NOT found in vocab\n",
            "เมื่ : NOT found in vocab\n",
            "เมื่อ : NOT found in vocab\n",
            "เมื่อว : NOT found in vocab\n",
            "เมื่อวา : NOT found in vocab\n",
            "เมื่อวาน : found in vocab \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ผม', 'นั่ง', 'รถ', 'ไฟฟ้า', 'ไป', 'สยาม', 'พารากอน', 'เมื่อวาน']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "More Practical vocab : เนื่องจาก vocab ที่เราโชว์ให้ดูก่อนหน้าเป็นแค่ส่งนนึงของ vocab จริงๆ เร็วๆนี้เราจะใช้ vocab ที่มาจาก pyThainlp ที่รวบรวมทุกคำมา"
      ],
      "metadata": {
        "id": "LhGNksTpFb-n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main Drawback : \n",
        "\n",
        "- sensitive กับคำสะกดผิด เช่น กิรข้าวยัง จะไม่สามารถ detect anything (ลามยาวไปถึงคำสุดท้าย)\n",
        "\n",
        "- vocab ต้องอัพเหตด้วยมือตลอดเวลาเนื่องด้วยความไม่เสถียรของภาษาวิบัติที่เปลี่ยนแปลงตลอดเวลา เช่น จุงเบย, ขก, สายมู (sol แบบ หยาบๆ: เพิ่มvocabพวกนั้น)\n",
        "\n",
        "- เนื่องด้วยการ adopt greedy search algorithm มาทำให้ไม่สามารถ detect คำแบบ 'น่าร้าก' , 'น่าร๊อก' เพราะคำว่า 'น่า' ก็เป็นคำที่มีความหมาย\n",
        "\n",
        "- หากมีคำอังกถษคำไทยคำ vocab size จะต้องเยอะขึ้น และ เราต้องแก้ไขปัญหาพวกนี้ทั้งไทยทั้งอังกถษ"
      ],
      "metadata": {
        "id": "LEeZ5YzOCa5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = ['เธอ','เก่ง','จัง','เลย','กะเพรา']\n",
        "test_txt = 'เทอเก่งจุงเบย'\n",
        "\n",
        "greedy_match_tokenizer(test_txt,vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uODKOHaWUuQ",
        "outputId": "7b14fd49-c74f-4f20-cda5-ea90c2461905"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "เ : NOT found in vocab\n",
            "เท : NOT found in vocab\n",
            "เทอ : NOT found in vocab\n",
            "เทอเ : NOT found in vocab\n",
            "เทอเก : NOT found in vocab\n",
            "เทอเก่ : NOT found in vocab\n",
            "เทอเก่ง : NOT found in vocab\n",
            "เทอเก่งจ : NOT found in vocab\n",
            "เทอเก่งจุ : NOT found in vocab\n",
            "เทอเก่งจุง : NOT found in vocab\n",
            "เทอเก่งจุงเ : NOT found in vocab\n",
            "เทอเก่งจุงเบ : NOT found in vocab\n",
            "เทอเก่งจุงเบย : NOT found in vocab\n",
            "=== Cannot detected ===\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<UNK>']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = ['เธอ','น่า','น่ากิน','น่ารัก','น่าเล่น','รัก']\n",
        "test_txt = 'เธอน่ารัก'\n",
        "\n",
        "greedy_match_tokenizer(test_txt,vocab)\n",
        "# Expected token_lst : ['เธอ','น่ารัก']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5blX3GPDmGQ",
        "outputId": "9286987a-acc1-45d1-f9d6-2034f11e6e08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "เ : NOT found in vocab\n",
            "เธ : NOT found in vocab\n",
            "เธอ : found in vocab \n",
            "\n",
            "น : NOT found in vocab\n",
            "น่ : NOT found in vocab\n",
            "น่า : found in vocab \n",
            "\n",
            "ร : NOT found in vocab\n",
            "รั : NOT found in vocab\n",
            "รัก : found in vocab \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['เธอ', 'น่า', 'รัก']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = ['กะเพรา','น่ากิน','น่ารัก','น่าเล่น','รัก','เลย','มาก']\n",
        "test_txt = 'กะเพราน่้ากินมากเลย'\n",
        "\n",
        "greedy_match_tokenizer(test_txt,vocab)\n",
        "# Expected token_lst : ['กะเพรา','น่้า','กิน','มาก','เลย'] "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbelfMXlb05S",
        "outputId": "e6383bf6-4004-4c7f-e901-ff3853cec927"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ก : NOT found in vocab\n",
            "กะ : NOT found in vocab\n",
            "กะเ : NOT found in vocab\n",
            "กะเพ : NOT found in vocab\n",
            "กะเพร : NOT found in vocab\n",
            "กะเพรา : found in vocab \n",
            "\n",
            "น : NOT found in vocab\n",
            "น่ : NOT found in vocab\n",
            "น่้ : NOT found in vocab\n",
            "น่้า : NOT found in vocab\n",
            "น่้าก : NOT found in vocab\n",
            "น่้ากิ : NOT found in vocab\n",
            "น่้ากิน : NOT found in vocab\n",
            "น่้ากินม : NOT found in vocab\n",
            "น่้ากินมา : NOT found in vocab\n",
            "น่้ากินมาก : NOT found in vocab\n",
            "น่้ากินมากเ : NOT found in vocab\n",
            "น่้ากินมากเล : NOT found in vocab\n",
            "น่้ากินมากเลย : NOT found in vocab\n",
            "=== Cannot detected ===\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['กะเพรา', '<UNK>']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Anyway, ในโลกความเป็นจริง vocab size ควรมากกว่านี้, เราสามารถโหลด vocab จำนวนใหญ่ได้ใน package pythainlp\n",
        "\n"
      ],
      "metadata": {
        "id": "vT5DMcP1d3HI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install pythainlp"
      ],
      "metadata": {
        "id": "uExF-JMNeAh2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f79fcac-af44-478f-aabd-010c6ef7a3ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pythainlp.corpus.common import thai_words\n",
        "vocab = list(thai_words())\n",
        "print(f'vocab size : {len(vocab)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPKR3Suud95N",
        "outputId": "115296d8-e4bc-40a5-d5af-a6022bb9e97d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size : 62055\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# มาดู 10 คำแรกกัน\n",
        "\n",
        "vocab[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38yNKKk8eW7n",
        "outputId": "2908e90a-ef79-4ebc-9326-8a1464411bd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['บัณฑรนาค',\n",
              " 'สามสิบกลีบ',\n",
              " 'ดุสิตรีสอร์ท',\n",
              " 'ประสูต',\n",
              " 'หัตถาโรหะ',\n",
              " 'เหมือนว่า',\n",
              " 'ค่าหด',\n",
              " 'ชุษณปักษ์',\n",
              " 'อวเคราะห์',\n",
              " 'ราตรี']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# มาลองกับ test_txt อะไรก็ได้ของเรากัน\n",
        "\n",
        "test_txt = 'ผมว่าผมต้องกินข้าวผัดทอดร้อนๆที่กรุงเทพมหานคร'\n",
        "\n",
        "greedy_match_tokenizer(test_txt,vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3387IdXedLu",
        "outputId": "57cbd6d8-cfa2-4784-db59-54aef457a1ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ผ : NOT found in vocab\n",
            "ผม : found in vocab \n",
            "\n",
            "ว : NOT found in vocab\n",
            "ว่ : NOT found in vocab\n",
            "ว่า : found in vocab \n",
            "\n",
            "ผ : NOT found in vocab\n",
            "ผม : found in vocab \n",
            "\n",
            "ต : NOT found in vocab\n",
            "ต้ : NOT found in vocab\n",
            "ต้อ : found in vocab \n",
            "\n",
            "ง : NOT found in vocab\n",
            "งก : found in vocab \n",
            "\n",
            "ิ : NOT found in vocab\n",
            "ิน : NOT found in vocab\n",
            "ินข : NOT found in vocab\n",
            "ินข้ : NOT found in vocab\n",
            "ินข้า : NOT found in vocab\n",
            "ินข้าว : NOT found in vocab\n",
            "ินข้าวผ : NOT found in vocab\n",
            "ินข้าวผั : NOT found in vocab\n",
            "ินข้าวผัด : NOT found in vocab\n",
            "ินข้าวผัดท : NOT found in vocab\n",
            "ินข้าวผัดทอ : NOT found in vocab\n",
            "ินข้าวผัดทอด : NOT found in vocab\n",
            "ินข้าวผัดทอดร : NOT found in vocab\n",
            "ินข้าวผัดทอดร้ : NOT found in vocab\n",
            "ินข้าวผัดทอดร้อ : NOT found in vocab\n",
            "ินข้าวผัดทอดร้อน : NOT found in vocab\n",
            "ินข้าวผัดทอดร้อนๆ : NOT found in vocab\n",
            "ินข้าวผัดทอดร้อนๆท : NOT found in vocab\n",
            "ินข้าวผัดทอดร้อนๆที : NOT found in vocab\n",
            "ินข้าวผัดทอดร้อนๆที่ : NOT found in vocab\n",
            "ินข้าวผัดทอดร้อนๆที่ก : NOT found in vocab\n",
            "ินข้าวผัดทอดร้อนๆที่กร : NOT found in vocab\n",
            "ินข้าวผัดทอดร้อนๆที่กรุ : NOT found in vocab\n",
            "ินข้าวผัดทอดร้อนๆที่กรุง : NOT found in vocab\n",
            "ินข้าวผัดทอดร้อนๆที่กรุงเ : NOT found in vocab\n",
            "ินข้าวผัดทอดร้อนๆที่กรุงเท : NOT found in vocab\n",
            "ินข้าวผัดทอดร้อนๆที่กรุงเทพ : NOT found in vocab\n",
            "ินข้าวผัดทอดร้อนๆที่กรุงเทพม : NOT found in vocab\n",
            "ินข้าวผัดทอดร้อนๆที่กรุงเทพมห : NOT found in vocab\n",
            "ินข้าวผัดทอดร้อนๆที่กรุงเทพมหา : NOT found in vocab\n",
            "ินข้าวผัดทอดร้อนๆที่กรุงเทพมหาน : NOT found in vocab\n",
            "ินข้าวผัดทอดร้อนๆที่กรุงเทพมหานค : NOT found in vocab\n",
            "ินข้าวผัดทอดร้อนๆที่กรุงเทพมหานคร : NOT found in vocab\n",
            "=== Cannot detected ===\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ผม', 'ว่า', 'ผม', 'ต้อ', 'งก', '<UNK>']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "จะเห็นว่า algorithm นี้ไม่ดีเนื่องจากข้อเสียจาก greedy search algorithm"
      ],
      "metadata": {
        "id": "IG5nwnn9e5RP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ilcih-ZkJWLQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅ Maximal First vocab match (Maximal Matching)"
      ],
      "metadata": {
        "id": "u5inhYFX2sqy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to greedy first vocabulary match, but instead of splitting the token upon detection, it continues until the longest word is found. The first step is trying starting with first character will attempt to slice until the last character."
      ],
      "metadata": {
        "id": "ryNkaD593G97"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The possible worst case of time complexity would be when every token contain 1 characters (time complexity = O(N!) )"
      ],
      "metadata": {
        "id": "uDFBVKQ15Wlb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ref : https://medium.com/@anshul16/maximum-matching-word-segmentation-algorithm-python-code-3444fe4bd6f9"
      ],
      "metadata": {
        "id": "wbVL_t4x6K4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pythainlp.corpus.common import thai_words\n",
        "vocab = set(thai_words()) # Use set is more practical than list to avoid duplicate vocab"
      ],
      "metadata": {
        "id": "58ZYh-TacAIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "มาลองคำนี้ดีกว่า 'หมาป่าน่ารักมาก' <br>\n",
        "เรา expect ว่า ['หมาป่า','น่ารัก','มาก']\n"
      ],
      "metadata": {
        "id": "pZEpMH7M6Gwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_txt = 'หมาป่าน่ารักมาก'"
      ],
      "metadata": {
        "id": "vl_o7zEd6vQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ถ้าลอง algorithm แรก จะไม่ได้ตามต้องการ\n",
        "\n",
        "greedy_match_tokenizer(test_txt, vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovHoXbyv6krb",
        "outputId": "1a699c08-d8f7-4377-d38d-b55a922a067d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ห : NOT found in vocab\n",
            "หม : NOT found in vocab\n",
            "หมา : found in vocab \n",
            "\n",
            "ป : NOT found in vocab\n",
            "ป่ : NOT found in vocab\n",
            "ป่า : found in vocab \n",
            "\n",
            "น : NOT found in vocab\n",
            "น่ : NOT found in vocab\n",
            "น่า : found in vocab \n",
            "\n",
            "ร : NOT found in vocab\n",
            "รั : NOT found in vocab\n",
            "รัก : found in vocab \n",
            "\n",
            "ม : NOT found in vocab\n",
            "มา : found in vocab \n",
            "\n",
            "ก : NOT found in vocab\n",
            "=== Cannot detected ===\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['หมา', 'ป่า', 'น่า', 'รัก', 'มา', '<UNK>']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# มาลองสร้าง maximal matching กัน\n",
        "\n",
        "token_lst = []\n",
        "\n",
        "skip_ = 1\n",
        "i = 0\n",
        "\n",
        "while i < len(test_txt):  # i : start from word position i_th\n",
        "\n",
        "    current_max_word = None\n",
        "\n",
        "    for j in range(len(test_txt)-i): # j : try from 0 until the position which match with vocab\n",
        "        \n",
        "        txt = test_txt[i:i+j+1]\n",
        "        if txt in vocab:\n",
        "            \"\"\"เปลี่ยนนิดหน่อย จาก algorithm ของ greedy match\n",
        "            1. เราจะยังไม่ append ทันที จนกว่าจะ slice ไปถึง last character\n",
        "            2. ไม่ break หลัง detect คำ\n",
        "            \"\"\"\n",
        "            current_max_word = txt\n",
        "            skip_ = len(current_max_word)\n",
        "            print(f'{txt} : found in vocab')\n",
        "        else:\n",
        "            print(f'{txt} : not found in vocab')\n",
        "            \n",
        "\n",
        "    if current_max_word:\n",
        "        print(f'Last detected word -> {current_max_word}  \\n\\n')\n",
        "        token_lst.append(current_max_word)\n",
        "        i += skip_\n",
        "        \n",
        "    else:\n",
        "        token_lst.append('<unk>')\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNzHQQRg5_Kv",
        "outputId": "3d27216c-cf3b-4f5f-90d8-1c201786a348"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ห : not found in vocab\n",
            "หม : not found in vocab\n",
            "หมา : found in vocab\n",
            "หมาป : not found in vocab\n",
            "หมาป่ : not found in vocab\n",
            "หมาป่า : found in vocab\n",
            "หมาป่าน : not found in vocab\n",
            "หมาป่าน่ : not found in vocab\n",
            "หมาป่าน่า : not found in vocab\n",
            "หมาป่าน่าร : not found in vocab\n",
            "หมาป่าน่ารั : not found in vocab\n",
            "หมาป่าน่ารัก : not found in vocab\n",
            "หมาป่าน่ารักม : not found in vocab\n",
            "หมาป่าน่ารักมา : not found in vocab\n",
            "หมาป่าน่ารักมาก : not found in vocab\n",
            "Last detected word -> หมาป่า  \n",
            "\n",
            "\n",
            "น : not found in vocab\n",
            "น่ : not found in vocab\n",
            "น่า : found in vocab\n",
            "น่าร : not found in vocab\n",
            "น่ารั : not found in vocab\n",
            "น่ารัก : found in vocab\n",
            "น่ารักม : not found in vocab\n",
            "น่ารักมา : not found in vocab\n",
            "น่ารักมาก : not found in vocab\n",
            "Last detected word -> น่ารัก  \n",
            "\n",
            "\n",
            "ม : not found in vocab\n",
            "มา : found in vocab\n",
            "มาก : found in vocab\n",
            "Last detected word -> มาก  \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_lst"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NOJ8R2o-hwW",
        "outputId": "8e64dd02-42b9-4d66-a6f7-108f88a47b7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['หมาป่า', 'น่ารัก', 'มาก']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "มา form เป็น defined function กันเลย "
      ],
      "metadata": {
        "id": "PiklQGGk_gez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def maximal_match_tokenizer(test_txt,vocab):\n",
        "    token_lst = []\n",
        "\n",
        "    skip_ = 1\n",
        "    i = 0\n",
        "\n",
        "    while i < len(test_txt):  # i : start from word position i_th\n",
        "\n",
        "        current_max_word = None\n",
        "\n",
        "        for j in range(len(test_txt)-i): # j : try from 0 until the position which match with vocab\n",
        "            \n",
        "            txt = test_txt[i:i+j+1]\n",
        "            if txt in vocab:\n",
        "                \"\"\"เปลี่ยนนิดหน่อย จาก algorithm ของ greedy match\n",
        "                1. เราจะยังไม่ append ทันที จนกว่าจะ slice ไปถึง last character\n",
        "                2. ไม่ break หลัง detect คำ\n",
        "                \"\"\"\n",
        "                current_max_word = txt\n",
        "                skip_ = len(current_max_word)\n",
        "                print(f'{txt} : found in vocab')\n",
        "            else:\n",
        "                print(f'{txt} : not found in vocab')\n",
        "                \n",
        "\n",
        "        if current_max_word:\n",
        "            print(f'Last detected word -> {current_max_word}  \\n\\n')\n",
        "            token_lst.append(current_max_word)\n",
        "            i += skip_\n",
        "            \n",
        "        else:\n",
        "            token_lst.append('<unk>')\n",
        "            break\n",
        "\n",
        "    return token_lst"
      ],
      "metadata": {
        "id": "mWHyUY6t_eea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = set(thai_words()) \n",
        "test_txt = 'หมาป่าน่ารักมาก'\n",
        "maximal_match_tokenizer(test_txt,vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPRjs2qaAOhw",
        "outputId": "c7c54085-7803-4a2e-de2f-a6bbfe6449de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ห : not found in vocab\n",
            "หม : not found in vocab\n",
            "หมา : found in vocab\n",
            "หมาป : not found in vocab\n",
            "หมาป่ : not found in vocab\n",
            "หมาป่า : found in vocab\n",
            "หมาป่าน : not found in vocab\n",
            "หมาป่าน่ : not found in vocab\n",
            "หมาป่าน่า : not found in vocab\n",
            "หมาป่าน่าร : not found in vocab\n",
            "หมาป่าน่ารั : not found in vocab\n",
            "หมาป่าน่ารัก : not found in vocab\n",
            "หมาป่าน่ารักม : not found in vocab\n",
            "หมาป่าน่ารักมา : not found in vocab\n",
            "หมาป่าน่ารักมาก : not found in vocab\n",
            "Last detected word -> หมาป่า  \n",
            "\n",
            "\n",
            "น : not found in vocab\n",
            "น่ : not found in vocab\n",
            "น่า : found in vocab\n",
            "น่าร : not found in vocab\n",
            "น่ารั : not found in vocab\n",
            "น่ารัก : found in vocab\n",
            "น่ารักม : not found in vocab\n",
            "น่ารักมา : not found in vocab\n",
            "น่ารักมาก : not found in vocab\n",
            "Last detected word -> น่ารัก  \n",
            "\n",
            "\n",
            "ม : not found in vocab\n",
            "มา : found in vocab\n",
            "มาก : found in vocab\n",
            "Last detected word -> มาก  \n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['หมาป่า', 'น่ารัก', 'มาก']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_txt = 'หมอดินกินไรนกฟิ'\n",
        "maximal_match_tokenizer(test_txt,vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMc3DnPcAxds",
        "outputId": "43943d2f-4ae1-42c4-ff09-794b5ec9d291"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ห : not found in vocab\n",
            "หม : not found in vocab\n",
            "หมอ : found in vocab\n",
            "หมอด : not found in vocab\n",
            "หมอดิ : not found in vocab\n",
            "หมอดิน : not found in vocab\n",
            "หมอดินก : not found in vocab\n",
            "หมอดินกิ : not found in vocab\n",
            "หมอดินกิน : not found in vocab\n",
            "หมอดินกินไ : not found in vocab\n",
            "หมอดินกินไร : not found in vocab\n",
            "หมอดินกินไรน : not found in vocab\n",
            "หมอดินกินไรนก : not found in vocab\n",
            "หมอดินกินไรนกฟ : not found in vocab\n",
            "หมอดินกินไรนกฟิ : not found in vocab\n",
            "Last detected word -> หมอ  \n",
            "\n",
            "\n",
            "ด : not found in vocab\n",
            "ดิ : found in vocab\n",
            "ดิน : found in vocab\n",
            "ดินก : not found in vocab\n",
            "ดินกิ : not found in vocab\n",
            "ดินกิน : not found in vocab\n",
            "ดินกินไ : not found in vocab\n",
            "ดินกินไร : not found in vocab\n",
            "ดินกินไรน : not found in vocab\n",
            "ดินกินไรนก : not found in vocab\n",
            "ดินกินไรนกฟ : not found in vocab\n",
            "ดินกินไรนกฟิ : not found in vocab\n",
            "Last detected word -> ดิน  \n",
            "\n",
            "\n",
            "ก : not found in vocab\n",
            "กิ : not found in vocab\n",
            "กิน : found in vocab\n",
            "กินไ : not found in vocab\n",
            "กินไร : not found in vocab\n",
            "กินไรน : not found in vocab\n",
            "กินไรนก : not found in vocab\n",
            "กินไรนกฟ : not found in vocab\n",
            "กินไรนกฟิ : not found in vocab\n",
            "Last detected word -> กิน  \n",
            "\n",
            "\n",
            "ไ : not found in vocab\n",
            "ไร : found in vocab\n",
            "ไรน : not found in vocab\n",
            "ไรนก : not found in vocab\n",
            "ไรนกฟ : not found in vocab\n",
            "ไรนกฟิ : not found in vocab\n",
            "Last detected word -> ไร  \n",
            "\n",
            "\n",
            "น : not found in vocab\n",
            "นก : found in vocab\n",
            "นกฟ : not found in vocab\n",
            "นกฟิ : not found in vocab\n",
            "Last detected word -> นก  \n",
            "\n",
            "\n",
            "ฟ : not found in vocab\n",
            "ฟิ : not found in vocab\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['หมอ', 'ดิน', 'กิน', 'ไร', 'นก', '<unk>']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main Drawback:\n",
        "1. algorithm เชื่อว่า token ที่ดีที่สุดคือ token ที่ต้องยาวที่สุดเท่าที่เป็นไปได้ แต่ในความเป็นจริง มันไม่จำเป็นต้องเป็นนั้นเสมอไป แล้วเราจะรู้ได้ไงหละ ว่า token ไหนดีสุด ด้วยเหตุนี้เราต้องใช้ Probability Theory มาเพื่อวัด score ของแต่ token candidate\n",
        "\n",
        "2. ใช้เวลานานหาก token ที่พบมันสั้นมาก "
      ],
      "metadata": {
        "id": "EyJ1hkVL_iJT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "q43Wj-jAFZr-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ❌ N-gram model "
      ],
      "metadata": {
        "id": "e2omPX8uXDGp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ปัญหาท่เจอก่อนหน้านี้จาก maximal matching คือ การ assume ว่า token ยิ่งยาวยิ่งดีกว่าเสมอ ซึ่งไม่จริงเสมอไป\n",
        "\n",
        "การที่เราจะแยกได้ว่าอะไรดีกว่าอาจจะต้องมีการ annotate score ของแต่ละ token ซึ่ง score ก็จะวัดจาก frequency ที่พบใน corpus (ที่เราเก็บมา)\n",
        "\n",
        "\n",
        "หากเราทำดังกล่าวก็จะเป็น Unigram\n",
        "\n",
        "แต่หากเราทำโดยให้ score ของ token โดยดูว่ามีโอกาสเกิดขึ้นกับ token ก่อนหน้าไหม ก็จะได้อีก score นึง "
      ],
      "metadata": {
        "id": "5zSZsWctXHwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unigram vocab\n",
        "\n",
        "vocab_unigram = {'เท' : 180 ,'กิน' : 300, \n",
        "                 'กินูริ' : 1 , 'ข้า' : 10,\n",
        "                 'ยัง' : 150,\n",
        "                 'เทอ' : 200}"
      ],
      "metadata": {
        "id": "3NJoHN3kXHOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bd7035WF7gI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "algorithm จะทำเหมือนกับ maximal matching แต่จะเก็บทุก candidate ที่ found ใน vocab และเลือกอันที่ score มากที่สุด"
      ],
      "metadata": {
        "id": "0JLs7fgSYdyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# มาลอง test คำว่า 'เทอกินข้าวยัง' # \n",
        "test_txt = 'เทอกินข้าวยัง'"
      ],
      "metadata": {
        "id": "Sea7tp6bZLzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is not UNIGRAM since we have to try both เท and เทอ \n",
        "\n",
        "token_lst = []\n",
        "\n",
        "skip_ = 1\n",
        "i = 0\n",
        "\n",
        "while i < len(test_txt):  # i : start from word position i_th\n",
        "\n",
        "    current_max_word   = None\n",
        "    max_p_txt = 0\n",
        "    for j in range(len(test_txt)-i): # j : try from 0 until the position which match with vocab\n",
        "        \n",
        "        txt = test_txt[i:i+j+1]\n",
        "        if txt in vocab_unigram:\n",
        "            \"\"\"เปลี่ยนนิดหน่อย จาก algorithm ของ maximal match\n",
        "            1. append ทุกๆ candidate ที่ found in vocab\n",
        "            \"\"\"\n",
        "            p_txt = vocab_unigram.get(txt)\n",
        "            if p_txt > max_p_txt:\n",
        "                current_max_word = txt\n",
        "                max_p_txt = p_txt\n",
        "            \n",
        "            skip_ = len(current_max_word)\n",
        "            print(f'{txt} : found in vocab')\n",
        "        else:\n",
        "            print(f'{txt} : not found in vocab')\n",
        "            \n",
        "\n",
        "    if current_max_word:\n",
        "        print(f'Last detected word -> {current_max_word}  \\n\\n')\n",
        "        token_lst.append(current_max_word)\n",
        "        i += skip_\n",
        "        \n",
        "    else:\n",
        "        token_lst.append('<unk>')\n",
        "        break\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rpm1JZLDYMJo",
        "outputId": "6a2be9f7-c570-4ef3-801d-b5cf8be90e73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "เ : not found in vocab\n",
            "เท : found in vocab\n",
            "เทอ : found in vocab\n",
            "เทอก : not found in vocab\n",
            "เทอกิ : not found in vocab\n",
            "เทอกิน : not found in vocab\n",
            "เทอกินข : not found in vocab\n",
            "เทอกินข้ : not found in vocab\n",
            "เทอกินข้า : not found in vocab\n",
            "เทอกินข้าว : not found in vocab\n",
            "เทอกินข้าวย : not found in vocab\n",
            "เทอกินข้าวยั : not found in vocab\n",
            "เทอกินข้าวยัง : not found in vocab\n",
            "Last detected word -> เทอ  \n",
            "\n",
            "\n",
            "ก : not found in vocab\n",
            "กิ : not found in vocab\n",
            "กิน : found in vocab\n",
            "กินข : not found in vocab\n",
            "กินข้ : not found in vocab\n",
            "กินข้า : not found in vocab\n",
            "กินข้าว : not found in vocab\n",
            "กินข้าวย : not found in vocab\n",
            "กินข้าวยั : not found in vocab\n",
            "กินข้าวยัง : not found in vocab\n",
            "Last detected word -> กิน  \n",
            "\n",
            "\n",
            "ข : not found in vocab\n",
            "ข้ : not found in vocab\n",
            "ข้า : found in vocab\n",
            "ข้าว : not found in vocab\n",
            "ข้าวย : not found in vocab\n",
            "ข้าวยั : not found in vocab\n",
            "ข้าวยัง : not found in vocab\n",
            "Last detected word -> ข้า  \n",
            "\n",
            "\n",
            "ว : not found in vocab\n",
            "วย : not found in vocab\n",
            "วยั : not found in vocab\n",
            "วยัง : not found in vocab\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_lst"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fASwY7m7jy_",
        "outputId": "e52846b1-2a50-4e27-c94f-33bb53d3d8be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['เทอ', 'กิน', 'ข้า', '<unk>']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ❌ Maximal Matching + Thai Character Cluster (TCC)"
      ],
      "metadata": {
        "id": "GskYVXdpG3yr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ปัจจุบัน มี tokenizer ไทยที่ถูกใช้อย่างกว้างขวาง อย่าง *newmm* ใช้ concept ของ maximal matching ผสมกับ Thai Character Cluster (TCC)"
      ],
      "metadata": {
        "id": "Bv6uDxFpG5eY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thai Character Cluster คืออะไร : https://dl.acm.org/doi/pdf/10.1145/355214.355225"
      ],
      "metadata": {
        "id": "FF2j11XPVMgT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "นี่ไม่ใช่ word tokenizer แต่เป็น sub-word tokenizer"
      ],
      "metadata": {
        "id": "l4u-YF-ST43-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ref: https://github.com/wisesight/newmm-tokenizer/blob/main/newmm_tokenizer/newmm.py"
      ],
      "metadata": {
        "id": "thWScKSdNIhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from typing import List, Set"
      ],
      "metadata": {
        "id": "Ue3txJDTNPRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_RE_TCC = (\n",
        "    \"\"\"\\\n",
        "เc็c\n",
        "เcctาะ\n",
        "เccีtยะ\n",
        "เccีtย(?=[เ-ไก-ฮ]|$)\n",
        "เcc็c\n",
        "เcิc์c\n",
        "เcิtc\n",
        "เcีtยะ?\n",
        "เcืtอะ?\n",
        "เc[ิีุู]tย(?=[เ-ไก-ฮ]|$)\n",
        "เctา?ะ?\n",
        "cัtวะ\n",
        "c[ัื]tc[ุิะ]?\n",
        "c[ิุู]์\n",
        "c[ะ-ู]t\n",
        "c็\n",
        "ct[ะาำ]?\n",
        "แc็c\n",
        "แcc์\n",
        "แctะ\n",
        "แcc็c\n",
        "แccc์\n",
        "โctะ\n",
        "[เ-ไ]ct\n",
        "\"\"\".replace(\n",
        "        \"c\", \"[ก-ฮ]\"\n",
        "    )\n",
        "    .replace(\"t\", \"[่-๋]?\")\n",
        "    .split()\n",
        ")\n",
        "\n",
        "_PAT_TCC = re.compile(\"|\".join(_RE_TCC))"
      ],
      "metadata": {
        "id": "YlI1P--QTnKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eDo2V3EftDzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# อันนี้ https://github.com/wisesight/newmm-tokenizer/blob/main/newmm_tokenizer/tcc.py\n",
        "\n",
        "def tcc(text: str) -> str:\n",
        "    \"\"\"\n",
        "    TCC generator, generates Thai Character Clusters\n",
        "    :param str text: text to be tokenized to character clusters\n",
        "    :return: subwords (character clusters)\n",
        "    :rtype: Iterator[str]\n",
        "    \"\"\"\n",
        "    if not text or not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    len_text = len(text)\n",
        "    p = 0\n",
        "    while p < len_text:\n",
        "        m = _PAT_TCC.match(text[p:])\n",
        "        if m:\n",
        "            n = m.span()[1]\n",
        "        else:\n",
        "            n = 1\n",
        "        yield text[p : p + n]\n",
        "        p += n\n",
        "\n",
        "\n",
        "def tcc_pos(text: str) -> Set[int]:\n",
        "    \"\"\"\n",
        "    TCC positions\n",
        "    :param str text: text to be tokenized to character clusters\n",
        "    :return: list of the end position of subwords\n",
        "    :rtype: set[int]\n",
        "    \"\"\"\n",
        "    if not text or not isinstance(text, str):\n",
        "        return set()\n",
        "\n",
        "    p_set = set()\n",
        "    p = 0\n",
        "    for w in tcc(text):\n",
        "        p += len(w)\n",
        "        p_set.add(p)\n",
        "\n",
        "    return p_set\n",
        "\n",
        "\n",
        "def segment(text: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Subword segmentation\n",
        "    :param str text: text to be tokenized to character clusters\n",
        "    :return: list of subwords (character clusters), tokenized from the text\n",
        "    :rtype: list[str]\n",
        "    \"\"\"\n",
        "\n",
        "    return list(tcc(text))"
      ],
      "metadata": {
        "id": "VoBLJSJgG4xX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(tcc('ชอบเธอนะ'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2x0IofFNSPK",
        "outputId": "fec5e11b-17b3-4752-d72d-ae6a72f321e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ช', 'อ', 'บ', 'เธ', 'อ', 'นะ']"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ❌ Deepcut"
      ],
      "metadata": {
        "id": "pFTP__8Vm1Sp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "deepcut เป็น algorithm ที่มีความ state-of-art ใน overall"
      ],
      "metadata": {
        "id": "sCRgh0-immFq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "จากปัญหาดังกล่าว จะเห็นได้ว่าการทำ rule based เพื่อสร้าง tokenizer ที่ดีเป็นไปได้ยาก และ ด้วยเหตุผลนี้ state-of-art tokenizer ได้ adopt idea ของ train from data is better than rule based to handle with the above problem\n",
        "\n",
        "จาก ปัญหา นี้ หากเราสามารถได้ data จากหลายแหล่งเช่น social media, ราชการ etc."
      ],
      "metadata": {
        "id": "8H4Vr6WbJXcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://user-images.githubusercontent.com/1214890/58486992-14c1d880-8191-11e9-9122-8385750e06bd.png)"
      ],
      "metadata": {
        "id": "3gkBhSo-mwZA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ใช้ character embedding ในการ represent input"
      ],
      "metadata": {
        "id": "_NJw7H0am9gu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-CmO9FOIcyQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "W"
      ],
      "metadata": {
        "id": "GR6LffDfJVuo"
      }
    }
  ]
}